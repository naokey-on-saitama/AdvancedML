{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import metrics\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "from src.decorator import add_print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearnデータセットに収録されたiris(アヤメ)のデータセットをロードしてデータフレームを作成\n",
    "def load_iris_data():\n",
    "    data = load_iris()\n",
    "    x = pd.DataFrame(data[\"data\"],columns=data[\"feature_names\"])\n",
    "    y = pd.DataFrame(data[\"target\"],columns=[\"target\"])\n",
    "    return x, y\n",
    "\n",
    "# 手書き文字のデータセットをダウンロードして、実験用データを準備 (70000枚のうち7000枚を利用)\n",
    "def load_mnist_data():\n",
    "    data = fetch_openml('mnist_784', version=1)\n",
    "    _x = np.array(data['data'].astype(np.float32))\n",
    "    _y = np.array(data['target'].astype(np.int32))\n",
    "    _, x, _, y = train_test_split(_x, _y, test_size=0.1, random_state=1, stratify=_y)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 一括処理のためにモデルの辞書を作成\n",
    "model = {\n",
    "    # k近傍法のモデル\n",
    "    'kNN(k=3)':\n",
    "    KNeighborsClassifier(n_neighbors=3, # k を指定 (デフォルトは 5)\n",
    "                         weights='uniform',  # 距離を考慮しない(uniform:デフォルト)、する(distance)\n",
    "                         algorithm='auto', # 近傍点計算アルゴリズム (auto:デフォルト,ball_tree,kd_tree,brute)\n",
    "                         leaf_size=30,  # ball_tree,kd_tree指定時のリーフサイズの設定 (デフォルトは 30)\n",
    "                         p=2),  # 距離計算の次元 (2:デフォルト、1)\n",
    "    # svm (kernel=\"linear\", C=1.0) のモデル\n",
    "    'SVC(kernel=\"linear\", C=1)':\n",
    "    svm.SVC(kernel=\"linear\", C=1, max_iter=100000, verbose=True, random_state=1),\n",
    "    # svm (kernel=\"rbf\", C=1) のモデル\n",
    "    'SVC(kernel=\"rbf\", C=1)':\n",
    "    svm.SVC(kernel=\"rbf\", C=1, max_iter=100000, verbose=True, random_state=1),\n",
    "    # 決定木\n",
    "    'DecisionTree(max_depth=10)':\n",
    "    DecisionTreeClassifier(max_depth=10, # 木の深さの最大\n",
    "                                random_state=2), # 乱数シード\n",
    "    # ランダムフォレストのモデル\n",
    "    'randomforest(max_depth=10, n_estimators=10)':\n",
    "    RandomForestClassifier(max_depth=10, # 木の深さの最大\n",
    "                             n_estimators=10, # 木の数\n",
    "                             random_state=2), # 乱数シード\n",
    "    # アダブーストのモデル\n",
    "    'Adaboost(dct(max_depth=10), n_estimators=170)':\n",
    "    AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=10, random_state=1), # ベースモデルを指定\n",
    "                            n_estimators=170, # 木の数\n",
    "                            random_state=1), # 乱数シード\n",
    "    # 勾配ブースティングのモデル\n",
    "    'GradientBoostingClassifier(max_depth=5, n_estimators=170)':\n",
    "    GradientBoostingClassifier(max_depth=5, # 木の深さの最大\n",
    "                               n_estimators=170, # 木の数\n",
    "                               random_state=1), # 乱数シード\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## dataset:Iris  x_train:112 x_test:38 y_train:112 y_test:38\n",
      "# with scaling\n",
      "dataset:Iris model:kNN(k=3) accuracy_score: train_data: 0.95536 test_data: 0.94737\n",
      "[LibSVM]dataset:Iris model:SVC(kernel=\"linear\", C=1) accuracy_score: train_data: 0.97321 test_data: 0.97368\n",
      "[LibSVM]dataset:Iris model:SVC(kernel=\"rbf\", C=1) accuracy_score: train_data: 0.97321 test_data: 0.97368\n",
      "dataset:Iris model:DecisionTree(max_depth=10) accuracy_score: train_data: 1.0 test_data: 0.97368\n",
      "dataset:Iris model:randomforest(max_depth=10, n_estimators=10) accuracy_score: train_data: 0.99107 test_data: 0.97368\n",
      "dataset:Iris model:Adaboost(dct(max_depth=10), n_estimators=170) accuracy_score: train_data: 1.0 test_data: 0.97368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\human\\OneDrive - 埼玉大学\\学部4年\\04_後期授業\\機械学習特論\\python\\env\\lib\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset:Iris model:GradientBoostingClassifier(max_depth=5, n_estimators=170) accuracy_score: train_data: 1.0 test_data: 0.97368\n"
     ]
    }
   ],
   "source": [
    "dataset_key = \"Iris\"\n",
    "# データを学習用と検証用に分割\n",
    "x, y = load_iris_data()\n",
    "\n",
    "x_train, x_test, y_train, y_test = \\\n",
    "    train_test_split(x, y, test_size=0.25, random_state=1, stratify=y) # 検証用データに25%を割当て\n",
    "print(f'## dataset:{dataset_key} ',\n",
    "        f'x_train:{len(x_train)} x_test:{len(x_test)} y_train:{len(y_train)} y_test:{len(y_test)}')\n",
    "\n",
    "# データを標準化\n",
    "print('# with scaling')\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(x)\n",
    "x_train = scaler.transform(x_train)\n",
    "x_test = scaler.transform(x_test)\n",
    "\n",
    "# 辞書に格納したモデルそれぞれについて性能を測定\n",
    "for model_key in model.keys():\n",
    "    # 学習用データを利用してモデルを学習\n",
    "    clf = model[model_key]\n",
    "    clf = clf.fit(x_train, np.array(y_train).ravel()) \n",
    "\n",
    "    # 学習したモデルの性能(正答率)を学習用データと検証用データで評価\n",
    "    predict_train = clf.predict(x_train)\n",
    "    train_score = metrics.accuracy_score(y_train, predict_train)\n",
    "    predict_test = clf.predict(x_test)\n",
    "    test_score = metrics.accuracy_score(y_test, predict_test)\n",
    "    print(f'dataset:{dataset_key} model:{model_key}', \n",
    "        f'accuracy_score: train_data:{train_score: 0.5} test_data:{test_score: 0.5}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\human\\OneDrive - 埼玉大学\\学部4年\\04_後期授業\\機械学習特論\\python\\env\\lib\\site-packages\\sklearn\\datasets\\_openml.py:1022: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## dataset:MNIST  x_train:5250 x_test:1750 y_train:5250 y_test:1750\n",
      "# with scaling\n",
      "dataset:MNIST model:kNN(k=3) accuracy_score: train_data: 0.94571 test_data: 0.89943\n",
      "[LibSVM]dataset:MNIST model:SVC(kernel=\"linear\", C=1) accuracy_score: train_data: 1.0 test_data: 0.91371\n",
      "[LibSVM]dataset:MNIST model:SVC(kernel=\"rbf\", C=1) accuracy_score: train_data: 0.98457 test_data: 0.92686\n",
      "dataset:MNIST model:DecisionTree(max_depth=10) accuracy_score: train_data: 0.95619 test_data: 0.77486\n",
      "dataset:MNIST model:randomforest(max_depth=10, n_estimators=10) accuracy_score: train_data: 0.97962 test_data: 0.87886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\human\\OneDrive - 埼玉大学\\学部4年\\04_後期授業\\機械学習特論\\python\\env\\lib\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset:MNIST model:Adaboost(dct(max_depth=10), n_estimators=170) accuracy_score: train_data: 1.0 test_data: 0.93543\n",
      "dataset:MNIST model:GradientBoostingClassifier(max_depth=5, n_estimators=170) accuracy_score: train_data: 1.0 test_data: 0.93429\n"
     ]
    }
   ],
   "source": [
    "dataset_key = \"MNIST\"\n",
    "# データを学習用と検証用に分割\n",
    "x, y = load_mnist_data()\n",
    "\n",
    "x_train, x_test, y_train, y_test = \\\n",
    "    train_test_split(x, y, test_size=0.25, random_state=1, stratify=y) # 検証用データに25%を割当て\n",
    "print(f'## dataset:{dataset_key} ',\n",
    "        f'x_train:{len(x_train)} x_test:{len(x_test)} y_train:{len(y_train)} y_test:{len(y_test)}')\n",
    "\n",
    "# データを標準化\n",
    "print('# with scaling')\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(x)\n",
    "x_train = scaler.transform(x_train)\n",
    "x_test = scaler.transform(x_test)\n",
    "\n",
    "# 辞書に格納したモデルそれぞれについて性能を測定\n",
    "for model_key in model.keys():\n",
    "    # 学習用データを利用してモデルを学習\n",
    "    clf = model[model_key]\n",
    "    clf = clf.fit(x_train, np.array(y_train).ravel()) \n",
    "\n",
    "    # 学習したモデルの性能(正答率)を学習用データと検証用データで評価\n",
    "    predict_train = clf.predict(x_train)\n",
    "    train_score = metrics.accuracy_score(y_train, predict_train)\n",
    "    predict_test = clf.predict(x_test)\n",
    "    test_score = metrics.accuracy_score(y_test, predict_test)\n",
    "    print(f'dataset:{dataset_key} model:{model_key}', \n",
    "        f'accuracy_score: train_data:{train_score: 0.5} test_data:{test_score: 0.5}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\human\\OneDrive - 埼玉大学\\学部4年\\04_後期授業\\機械学習特論\\python\\env\\lib\\site-packages\\sklearn\\datasets\\_openml.py:1022: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## dataset:Fashion-MNIST  x_train:5250 x_test:1750 y_train:5250 y_test:1750\n",
      "# with scaling\n",
      "dataset:Fashion-MNIST model:kNN(k=3) accuracy_score: train_data: 0.89257 test_data: 0.79886\n",
      "[LibSVM]dataset:Fashion-MNIST model:SVC(kernel=\"linear\", C=1) accuracy_score: train_data: 1.0 test_data: 0.79943\n",
      "[LibSVM]dataset:Fashion-MNIST model:SVC(kernel=\"rbf\", C=1) accuracy_score: train_data: 0.9141 test_data: 0.82571\n",
      "dataset:Fashion-MNIST model:DecisionTree(max_depth=10) accuracy_score: train_data: 0.90895 test_data: 0.73029\n",
      "dataset:Fashion-MNIST model:randomforest(max_depth=10, n_estimators=10) accuracy_score: train_data: 0.94 test_data: 0.80343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\human\\OneDrive - 埼玉大学\\学部4年\\04_後期授業\\機械学習特論\\python\\env\\lib\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset:Fashion-MNIST model:Adaboost(dct(max_depth=10), n_estimators=170) accuracy_score: train_data: 1.0 test_data: 0.82857\n",
      "dataset:Fashion-MNIST model:GradientBoostingClassifier(max_depth=5, n_estimators=170) accuracy_score: train_data: 1.0 test_data: 0.83486\n"
     ]
    }
   ],
   "source": [
    "# Fashion-MNISTデータセットをダウンロードして、実験用データを準備 (70000枚のうち7000枚を利用)\n",
    "def load_fashion_mnist_data():\n",
    "    data = fetch_openml('Fashion-MNIST')\n",
    "    _x = np.array(data['data'].astype(np.float32))\n",
    "    _y = np.array(data['target'].astype(np.int32))\n",
    "    _, x, _, y = train_test_split(_x, _y, test_size=0.1, random_state=1, stratify=_y) \n",
    "    return x, y\n",
    "\n",
    "dataset_key = \"Fashion-MNIST\"\n",
    "# データを学習用と検証用に分割\n",
    "x, y = load_fashion_mnist_data()\n",
    "\n",
    "x_train, x_test, y_train, y_test = \\\n",
    "    train_test_split(x, y, test_size=0.25, random_state=1, stratify=y) # 検証用データに25%を割当て\n",
    "print(f'## dataset:{dataset_key} ',\n",
    "        f'x_train:{len(x_train)} x_test:{len(x_test)} y_train:{len(y_train)} y_test:{len(y_test)}')\n",
    "\n",
    "# データを標準化\n",
    "print('# with scaling')\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(x)\n",
    "x_train = scaler.transform(x_train)\n",
    "x_test = scaler.transform(x_test)\n",
    "\n",
    "# 辞書に格納したモデルそれぞれについて性能を測定\n",
    "for model_key in model.keys():\n",
    "    # 学習用データを利用してモデルを学習\n",
    "    clf = model[model_key]\n",
    "    clf = clf.fit(x_train, np.array(y_train).ravel()) \n",
    "\n",
    "    # 学習したモデルの性能(正答率)を学習用データと検証用データで評価\n",
    "    predict_train = clf.predict(x_train)\n",
    "    train_score = metrics.accuracy_score(y_train, predict_train)\n",
    "    predict_test = clf.predict(x_test)\n",
    "    test_score = metrics.accuracy_score(y_test, predict_test)\n",
    "    print(f'dataset:{dataset_key} model:{model_key}', \n",
    "        f'accuracy_score: train_data:{train_score: 0.5} test_data:{test_score: 0.5}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\human\\OneDrive - 埼玉大学\\学部4年\\04_後期授業\\機械学習特論\\python\\env\\lib\\site-packages\\sklearn\\datasets\\_openml.py:1022: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Total size: 7000\n",
      "Train size: 5250\n",
      "Test size : 1750\n",
      "\n",
      "\n",
      "# with scaling\n",
      "Test Accuracy\t: 0.8480\n",
      "Train Accuracy\t: 1.0000\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "x, y = load_fashion_mnist_data()\n",
    "\n",
    "x_train, x_test, y_train, y_test = \\\n",
    "    train_test_split(x, y, test_size=0.25, random_state=1, stratify=y) # 検証用データに25%を割当て\n",
    "\n",
    "print(\n",
    "f\"\"\"\n",
    "\n",
    "Total size: {len(x)}\n",
    "Train size: {len(x_train)}\n",
    "Test size : {len(x_test)}\n",
    "\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# データを標準化\n",
    "print('# with scaling')\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(x)\n",
    "x_train = scaler.transform(x_train)\n",
    "x_test = scaler.transform(x_test)\n",
    "\n",
    "lgb_train = lgb.Dataset(x_train, y_train)\n",
    "\n",
    "gbm = lgb.LGBMClassifier(\n",
    "    num_leaves=15,\n",
    "    max_depth=10,\n",
    "    n_estimators=170,\n",
    "    verbosity=-1\n",
    ")\n",
    "\n",
    "gbm.fit(x_train, y_train)\n",
    "p = gbm.predict(x_test)\n",
    "\n",
    "print(\"Test Accuracy\\t: {:.4f}\".format(metrics.accuracy_score(y_test, p)))\n",
    "print(\"Train Accuracy\\t: {:.4f}\".format(metrics.accuracy_score(y_train, gbm.predict(x_train))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Total size: 7000\n",
      "Train size: 5250\n",
      "Test size : 1750\n",
      "\n",
      "\n",
      "# with scaling\n",
      "\n",
      "==============================\n",
      "\n",
      "Test Accuracy\t: 0.8417\n",
      "Train Accuracy\t: 1.0000\n",
      "\n",
      "==============================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# データを正規化\n",
    "x_train, x_test, y_train, y_test = \\\n",
    "    train_test_split(x, y, test_size=0.25, random_state=1, stratify=y) # 検証用データに25%を割当て\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "lgb_train = lgb.Dataset(x_train, y_train)\n",
    "\n",
    "num_leaves = 31\n",
    "max_depth = -1\n",
    "n_estimators = 170\n",
    "\n",
    "print(\n",
    "f\"\"\"\n",
    "# Params\n",
    "- `num_leaves`: {num_leaves}\n",
    "- `max_depth`: {max_depth}\n",
    "- `n_estimators`: {n_estimators}\n",
    "- 正規化\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "gbm = lgb.LGBMClassifier(\n",
    "    num_leaves=num_leaves,\n",
    "    max_depth=max_depth,\n",
    "    n_estimators=n_estimators,\n",
    "    verbosity=-1\n",
    ")\n",
    "\n",
    "gbm.fit(x_train, y_train)\n",
    "p = gbm.predict(x_test)\n",
    "\n",
    "@add_print()\n",
    "def print_acc():\n",
    "    print(\"Test Accuracy\\t: {:.4f}\".format(metrics.accuracy_score(y_test, p)))\n",
    "    print(\"Train Accuracy\\t: {:.4f}\".format(metrics.accuracy_score(y_train, gbm.predict(x_train))))\n",
    "print_acc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# Params\n",
      "- `num_leaves`: 31\n",
      "- `max_depth`: -1\n",
      "- `n_estimators`: 170\n",
      "- 正規化\n",
      "\n",
      "\n",
      "==============================\n",
      "\n",
      "Test Accuracy\t: 0.8400\n",
      "Train Accuracy\t: 1.0000\n",
      "\n",
      "==============================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# データを正規化\n",
    "x_train, x_test, y_train, y_test = \\\n",
    "    train_test_split(x, y, test_size=0.25, random_state=1, stratify=y) # 検証用データに25%を割当て\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "lgb_train = lgb.Dataset(x_train, y_train)\n",
    "\n",
    "num_leaves = 31\n",
    "max_depth = -1\n",
    "n_estimators = 170\n",
    "\n",
    "print(\n",
    "f\"\"\"\n",
    "# Params\n",
    "- `num_leaves`: {num_leaves}\n",
    "- `max_depth`: {max_depth}\n",
    "- `n_estimators`: {n_estimators}\n",
    "- 正規化\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "gbm = lgb.LGBMClassifier(\n",
    "    num_leaves=num_leaves,\n",
    "    max_depth=max_depth,\n",
    "    n_estimators=n_estimators,\n",
    "    verbosity=-1\n",
    ")\n",
    "\n",
    "gbm.fit(x_train, y_train)\n",
    "p = gbm.predict(x_test)\n",
    "\n",
    "@add_print()\n",
    "def print_acc():\n",
    "    print(\"Test Accuracy\\t: {:.4f}\".format(metrics.accuracy_score(y_test, p)))\n",
    "    print(\"Train Accuracy\\t: {:.4f}\".format(metrics.accuracy_score(y_train, gbm.predict(x_train))))\n",
    "print_acc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# Params\n",
      "- `num_leaves`: 63\n",
      "- `max_depth`: -1\n",
      "- `n_estimators`: 170\n",
      "- 正規化\n",
      "\n",
      "\n",
      "==============================\n",
      "\n",
      "Test Accuracy\t: 0.8463\n",
      "Train Accuracy\t: 1.0000\n",
      "\n",
      "==============================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# データを正規化\n",
    "x_train, x_test, y_train, y_test = \\\n",
    "    train_test_split(x, y, test_size=0.25, random_state=1, stratify=y) # 検証用データに25%を割当て\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "lgb_train = lgb.Dataset(x_train, y_train)\n",
    "\n",
    "num_leaves = 63\n",
    "max_depth = -1\n",
    "n_estimators = 170\n",
    "\n",
    "print(\n",
    "f\"\"\"\n",
    "# Params\n",
    "- `num_leaves`: {num_leaves}\n",
    "- `max_depth`: {max_depth}\n",
    "- `n_estimators`: {n_estimators}\n",
    "- 正規化\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "gbm = lgb.LGBMClassifier(\n",
    "    num_leaves=num_leaves,\n",
    "    max_depth=max_depth,\n",
    "    n_estimators=n_estimators,\n",
    "    verbosity=-1\n",
    ")\n",
    "\n",
    "gbm.fit(x_train, y_train)\n",
    "p = gbm.predict(x_test)\n",
    "\n",
    "@add_print()\n",
    "def print_acc():\n",
    "    print(\"Test Accuracy\\t: {:.4f}\".format(metrics.accuracy_score(y_test, p)))\n",
    "    print(\"Train Accuracy\\t: {:.4f}\".format(metrics.accuracy_score(y_train, gbm.predict(x_train))))\n",
    "print_acc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# Params\n",
      "- `num_leaves`: 31\n",
      "- `max_depth`: -1\n",
      "- `n_estimators`: 100\n",
      "- 正規化\n",
      "\n",
      "\n",
      "==============================\n",
      "\n",
      "Test Accuracy\t: 0.8394\n",
      "Train Accuracy\t: 1.0000\n",
      "\n",
      "==============================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# データを正規化\n",
    "x_train, x_test, y_train, y_test = \\\n",
    "    train_test_split(x, y, test_size=0.25, random_state=1, stratify=y) # 検証用データに25%を割当て\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "lgb_train = lgb.Dataset(x_train, y_train)\n",
    "\n",
    "num_leaves = 31\n",
    "max_depth = -1\n",
    "n_estimators = 100\n",
    "\n",
    "print(\n",
    "f\"\"\"\n",
    "# Params\n",
    "- `num_leaves`: {num_leaves}\n",
    "- `max_depth`: {max_depth}\n",
    "- `n_estimators`: {n_estimators}\n",
    "- 正規化\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "gbm = lgb.LGBMClassifier(\n",
    "    num_leaves=num_leaves,\n",
    "    max_depth=max_depth,\n",
    "    n_estimators=n_estimators,\n",
    "    verbosity=-1\n",
    ")\n",
    "\n",
    "gbm.fit(x_train, y_train)\n",
    "p = gbm.predict(x_test)\n",
    "\n",
    "@add_print()\n",
    "def print_acc():\n",
    "    print(\"Test Accuracy\\t: {:.4f}\".format(metrics.accuracy_score(y_test, p)))\n",
    "    print(\"Train Accuracy\\t: {:.4f}\".format(metrics.accuracy_score(y_train, gbm.predict(x_train))))\n",
    "print_acc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# Params\n",
      "- Defalut\n",
      "- `num_leaves`: 31\n",
      "- `max_depth`: 10\n",
      "- `n_estimators`: 170\n",
      "- `scalign`: True\n",
      "\n",
      "\n",
      "==============================\n",
      "\n",
      "Test Accuracy\t: 0.8440\n",
      "Train Accuracy\t: 1.0000\n",
      "\n",
      "==============================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# データを正規化\n",
    "x_train, x_test, y_train, y_test = \\\n",
    "    train_test_split(x, y, test_size=0.25, random_state=1, stratify=y) # 検証用データに25%を割当て\n",
    "# x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "# lgb_train = lgb.Dataset(x_train, y_train)\n",
    "scaling = True\n",
    "num_leaves = 31\n",
    "max_depth = 10\n",
    "n_estimators = 170\n",
    "\n",
    "if scaling:\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(x)\n",
    "    x_train = scaler.transform(x_train)\n",
    "    x_test = scaler.transform(x_test)\n",
    "\n",
    "print(\n",
    "f\"\"\"\n",
    "# Params\n",
    "- Defalut\n",
    "- `num_leaves`: {num_leaves}\n",
    "- `max_depth`: {max_depth}\n",
    "- `n_estimators`: {n_estimators}\n",
    "- `scalign`: {scaling}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "gbm = lgb.LGBMClassifier(\n",
    "    # num_leaves=num_leaves,\n",
    "    # max_depth=max_depth,\n",
    "    # n_estimators=n_estimators,\n",
    "    verbosity=-1\n",
    ")\n",
    "\n",
    "gbm.fit(x_train, y_train)\n",
    "p = gbm.predict(x_test)\n",
    "\n",
    "@add_print()\n",
    "def print_acc():\n",
    "    print(\"Test Accuracy\\t: {:.4f}\".format(metrics.accuracy_score(y_test, p)))\n",
    "    print(\"Train Accuracy\\t: {:.4f}\".format(metrics.accuracy_score(y_train, gbm.predict(x_train))))\n",
    "print_acc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データを正規化\n",
    "x_train, x_test, y_train, y_test = \\\n",
    "    train_test_split(x, y, test_size=0.25, random_state=1, stratify=y) # 検証用データに25%を割当て\n",
    "# x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "lgb_train = lgb.Dataset(x_train, y_train)\n",
    "\n",
    "num_leaves = 31\n",
    "max_depth = 10\n",
    "n_estimators = 170\n",
    "\n",
    "print(\n",
    "f\"\"\"\n",
    "# Params\n",
    "- Defalut\n",
    "- `num_leaves`: {num_leaves}\n",
    "- `max_depth`: {max_depth}\n",
    "- `n_estimators`: {n_estimators}\n",
    "- `norm`: `False`\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "gbm = lgb.LGBMClassifier(\n",
    "    num_leaves=num_leaves,\n",
    "    max_depth=max_depth,\n",
    "    n_estimators=n_estimators,\n",
    "    verbosity=-1\n",
    ")\n",
    "\n",
    "gbm.fit(x_train, y_train)\n",
    "p = gbm.predict(x_test)\n",
    "\n",
    "@add_print()\n",
    "def print_acc():\n",
    "    print(\"Test Accuracy\\t: {:.4f}\".format(metrics.accuracy_score(y_test, p)))\n",
    "    print(\"Train Accuracy\\t: {:.4f}\".format(metrics.accuracy_score(y_train, gbm.predict(x_train))))\n",
    "print_acc()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
